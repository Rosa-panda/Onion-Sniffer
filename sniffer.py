#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ´‹è‘±å—…æ¢å™¨ - è‡ªåŠ¨å‘ç°å’Œçˆ¬å– .onion ç«™ç‚¹ï¼ˆå¼‚æ­¥ç‰ˆ + PostgreSQLï¼‰

ä¼˜åŒ–ç‚¹ï¼š
1. aiohttp å¼‚æ­¥è¯·æ±‚ï¼Œå¤§å¹…æå‡å¹¶å‘æ€§èƒ½
2. SimHash ç›¸ä¼¼åº¦æ£€æµ‹ï¼ˆé˜²æ­¢çˆ¬å–é•œåƒç«™/ç›¸ä¼¼é¡µé¢ï¼‰
3. å…³é”®è¯è¿‡æ»¤ï¼ˆåªä¿ç•™æŠ€æœ¯ç›¸å…³å†…å®¹ï¼‰
4. PostgreSQL åç«¯ï¼ˆæ”¯æŒè¶…å¤§æ•°æ®é‡ï¼‰
5. æ— é™çˆ¬å–æ¨¡å¼ï¼ˆmax_pages=0ï¼‰
"""
import re
import hashlib
import asyncio
import warnings
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
from collections import Counter
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))
from engine import OnionEngine

# è¿‡æ»¤ BeautifulSoup çš„ XML è­¦å‘Š
warnings.filterwarnings('ignore', category=XMLParsedAsHTMLWarning)

# ä»é…ç½®æ–‡ä»¶åŠ è½½
try:
    from config import PG_CONFIG
except ImportError:
    # é»˜è®¤é…ç½®ï¼ˆéœ€è¦åˆ›å»º config.pyï¼‰
    PG_CONFIG = {
        'host': 'localhost',
        'port': 5432,
        'database': 'onion_data',
        'user': 'postgres',
        'password': 'your_password_here'
    }
    print("[!] è­¦å‘Š: æœªæ‰¾åˆ° config.pyï¼Œè¯·å¤åˆ¶ config.example.py å¹¶é…ç½®")

# V3 æ´‹è‘±åœ°å€æ­£åˆ™ï¼ˆ56 ä½ base32 å­—ç¬¦ï¼‰
ONION_PATTERN = re.compile(r'[a-z2-7]{56}\.onion', re.IGNORECASE)

# æ¸…ç½‘é•œåƒç«™é»‘åå•ï¼ˆè¿™äº›ç«™ç‚¹çš„æ´‹è‘±ç‰ˆæœ¬æ²¡æœ‰ç‹¬ç‰¹å†…å®¹ï¼‰
CLEARNET_MIRRORS = {
    'wikipedia', 'wikimedia', 'wikidata', 'wikiquote', 'wikisource',
    'facebook', 'facebookcorewwwi', 'facebookwkhpilnemxj7asaniu7vnjjbiltxjqhye3m',
    'twitter', 'nytimes', 'bbc', 'cnn', 'theguardian',
    'protonmail', 'proton', 'tutanota',
    'duckduckgo', 'startpage', 'searx',
    'debian', 'ubuntu', 'archlinux', 'gentoo',
    'torproject', 'tails', 'whonix',
    'fsfe', 'eff', 'aclu',
    'ciadotgov', 'cia',
    'securedrop',  # è™½ç„¶æœ‰ä»·å€¼ä½†éƒ½æ˜¯æ¸…ç½‘åª’ä½“çš„å…¥å£
    'anarchistlibrary', 'theanarchistlibrary',
    'goodgame',  # æ¸¸æˆæœåŠ¡å™¨æ‰˜ç®¡
    'flibusta',  # ä¿„è¯­ç”µå­ä¹¦ï¼ˆæ¸…ç½‘æœ‰ï¼‰
}

# æŠ€æœ¯ç›¸å…³å…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤åƒåœ¾å†…å®¹ï¼‰
TECH_KEYWORDS = {
    'security', 'hacking', 'exploit', 'vulnerability', 'malware', 'reverse',
    'programming', 'code', 'linux', 'windows', 'kernel', 'binary', 'ctf',
    'crypto', 'encryption', 'forensic', 'pentest', 'research', 'tool',
    'github', 'git', 'python', 'rust', 'assembly', 'debug', 'analysis',
    'leak', 'database', 'dump', 'source', 'documentation', 'tutorial',
    'forum', 'community', 'wiki', 'library', 'archive', 'mirror',
}


class SimHash:
    """SimHash ç›¸ä¼¼åº¦æ£€æµ‹"""
    def __init__(self, hash_bits=64):
        self.hash_bits = hash_bits
    
    def _tokenize(self, text: str) -> list:
        return re.findall(r'\w+', text.lower())
    
    def compute(self, text: str) -> int:
        tokens = self._tokenize(text)
        if not tokens:
            return 0
        
        token_counts = Counter(tokens)
        v = [0] * self.hash_bits
        
        for token, count in token_counts.items():
            token_hash = int(hashlib.md5(token.encode()).hexdigest(), 16)
            for i in range(self.hash_bits):
                bit = (token_hash >> i) & 1
                if bit:
                    v[i] += count
                else:
                    v[i] -= count
        
        fingerprint = 0
        for i in range(self.hash_bits):
            if v[i] > 0:
                fingerprint |= (1 << i)
        return fingerprint
    
    def distance(self, hash1: int, hash2: int) -> int:
        return bin(hash1 ^ hash2).count('1')
    
    def is_similar(self, hash1: int, hash2: int, threshold: int = 10) -> bool:
        return self.distance(hash1, hash2) <= threshold


class OnionSniffer:
    """æ´‹è‘±ç½‘ç»œå—…æ¢å™¨ï¼ˆå¼‚æ­¥ç‰ˆ + PostgreSQLï¼‰"""
    
    def __init__(self, pg_config: dict = None):
        self.engine = OnionEngine()
        self.pg_config = pg_config or PG_CONFIG
        self.simhash = SimHash()
        
        # å†…å­˜ä¸­çš„å·²è®¿é—®é›†åˆ
        self.visited_urls = set()
        self.visited_hashes = set()
        self.simhashes = []
        
        # åŸŸåå¤±è´¥è®¡æ•°å™¨ï¼ˆè¿ç»­å¤±è´¥ N æ¬¡å°±è·³è¿‡ï¼‰
        self.domain_fail_count = {}
        self.blacklisted_domains = set()
        self.MAX_DOMAIN_FAILS = 3  # è¿ç»­å¤±è´¥ 3 æ¬¡å°±æ‹‰é»‘
        
        # å¼‚æ­¥é”ï¼ˆä¿æŠ¤å…±äº«çŠ¶æ€ï¼‰
        self._lock = asyncio.Lock()
        
        # æ•°æ®åº“è¿æ¥æ± ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._pool = None
    
    async def _get_pool(self):
        """è·å–æˆ–åˆ›å»º PostgreSQL è¿æ¥æ± """
        if self._pool is None:
            import asyncpg
            self._pool = await asyncpg.create_pool(
                host=self.pg_config['host'],
                port=self.pg_config['port'],
                database=self.pg_config['database'],
                user=self.pg_config['user'],
                password=self.pg_config['password'],
                min_size=2,
                max_size=10
            )
        return self._pool
    
    async def init_db(self):
        """åˆå§‹åŒ– PostgreSQL æ•°æ®åº“"""
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            # åˆ›å»ºè¡¨
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS onion_sites (
                    id SERIAL PRIMARY KEY,
                    domain TEXT UNIQUE,
                    first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_seen TIMESTAMP,
                    status TEXT DEFAULT 'pending',
                    priority INTEGER DEFAULT 0
                )
            ''')
            
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS pages (
                    id SERIAL PRIMARY KEY,
                    url TEXT UNIQUE,
                    domain TEXT,
                    title TEXT,
                    content_hash TEXT,
                    simhash TEXT,
                    content_type TEXT,
                    relevance_score REAL DEFAULT 0,
                    crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS documents (
                    id SERIAL PRIMARY KEY,
                    url TEXT UNIQUE,
                    filename TEXT,
                    content_type TEXT,
                    size INTEGER,
                    local_path TEXT,
                    downloaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•ï¼ˆåŠ é€ŸæŸ¥è¯¢ï¼‰
            await conn.execute('CREATE INDEX IF NOT EXISTS idx_pages_domain ON pages(domain)')
            await conn.execute('CREATE INDEX IF NOT EXISTS idx_pages_relevance ON pages(relevance_score)')
            await conn.execute('CREATE INDEX IF NOT EXISTS idx_pages_content_hash ON pages(content_hash)')
            await conn.execute('CREATE INDEX IF NOT EXISTS idx_sites_domain ON onion_sites(domain)')
        
        await self._load_visited()
    
    async def _load_visited(self):
        """ä»æ•°æ®åº“åŠ è½½å·²è®¿é—®çš„ URL"""
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            # åŠ è½½å·²è®¿é—® URL
            rows = await conn.fetch("SELECT url FROM pages")
            self.visited_urls = {row['url'] for row in rows}
            
            # åŠ è½½å†…å®¹å“ˆå¸Œ
            rows = await conn.fetch("SELECT content_hash FROM pages WHERE content_hash IS NOT NULL")
            self.visited_hashes = {row['content_hash'] for row in rows}
            
            # åŠ è½½ SimHashï¼ˆå­—ç¬¦ä¸²è½¬æ•´æ•°ï¼Œå¤„ç†ç§‘å­¦è®¡æ•°æ³•ï¼‰
            rows = await conn.fetch("SELECT simhash FROM pages WHERE simhash IS NOT NULL")
            self.simhashes = []
            for row in rows:
                if row['simhash']:
                    try:
                        # å¤„ç†ç§‘å­¦è®¡æ•°æ³•æ ¼å¼
                        self.simhashes.append(int(float(row['simhash'])))
                    except:
                        pass
        
        print(f"[*] å·²åŠ è½½ {len(self.visited_urls)} ä¸ªå·²è®¿é—® URLï¼Œ{len(self.simhashes)} ä¸ª SimHash")
    
    def _is_content_similar(self, simhash: int) -> bool:
        for existing_hash in self.simhashes:
            if self.simhash.is_similar(simhash, existing_hash, threshold=8):
                return True
        return False
    
    async def _record_domain_fail(self, domain: str):
        """è®°å½•åŸŸåå¤±è´¥ï¼Œè¿ç»­å¤±è´¥è¶…è¿‡é˜ˆå€¼å°±æ‹‰é»‘"""
        async with self._lock:
            self.domain_fail_count[domain] = self.domain_fail_count.get(domain, 0) + 1
            if self.domain_fail_count[domain] >= self.MAX_DOMAIN_FAILS:
                self.blacklisted_domains.add(domain)
                print(f"[ğŸš«] åŸŸåæ‹‰é»‘ï¼ˆè¿ç»­å¤±è´¥ {self.MAX_DOMAIN_FAILS} æ¬¡ï¼‰: {domain[:40]}")
    
    def _calculate_relevance(self, text: str, title: str) -> float:
        text_lower = (text + " " + title).lower()
        matches = sum(1 for kw in TECH_KEYWORDS if kw in text_lower)
        return min(matches / 5.0, 1.0)
    
    def extract_onion_links(self, html: str, base_url: str) -> set:
        links = set()
        
        for match in ONION_PATTERN.finditer(html):
            domain = match.group(0).lower()
            # è¿‡æ»¤åƒåœ¾åœ°å€ï¼ˆè¿ç»­é‡å¤å­—ç¬¦å¤ªå¤šçš„ï¼‰
            if self._is_junk_domain(domain):
                continue
            # è¿‡æ»¤æ¸…ç½‘é•œåƒç«™
            if self._is_clearnet_mirror(domain):
                continue
            links.add(f"http://{domain}/")
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.startswith(('gopher://', 'irc://', 'mailto:', 'javascript:', 'ftp://', 'magnet:')):
                    continue
                full_url = urljoin(base_url, href)
                if '.onion' in full_url and full_url.startswith(('http://', 'https://')):
                    domain = urlparse(full_url).netloc
                    # è¿‡æ»¤åƒåœ¾åŸŸåå’Œæ¸…ç½‘é•œåƒ
                    if self._is_junk_domain(domain) or self._is_clearnet_mirror(domain):
                        continue
                    links.add(full_url)
        except:
            pass
        
        return links
    
    def _is_junk_domain(self, domain: str) -> bool:
        """æ£€æµ‹åƒåœ¾åŸŸåï¼ˆè¿ç»­é‡å¤å­—ç¬¦å¤ªå¤šï¼‰"""
        # æå– .onion å‰çš„éƒ¨åˆ†
        name = domain.replace('.onion', '')
        if len(name) < 10:
            return True
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­ 10 ä¸ªä»¥ä¸Šç›¸åŒå­—ç¬¦
        for i in range(len(name) - 9):
            if len(set(name[i:i+10])) == 1:
                return True
        return False
    
    def _is_clearnet_mirror(self, domain: str) -> bool:
        """æ£€æµ‹æ¸…ç½‘é•œåƒç«™"""
        domain_lower = domain.lower()
        for mirror in CLEARNET_MIRRORS:
            if mirror in domain_lower:
                return True
        return False
    
    async def _save_page(self, data: dict):
        """ä¿å­˜é¡µé¢åˆ°æ•°æ®åº“"""
        # åªä¿å­˜ .onion åŸŸå
        if not data['domain'].endswith('.onion'):
            return
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            try:
                # SimHash è½¬å­—ç¬¦ä¸²å­˜å‚¨
                simhash_str = str(data['simhash']) if data.get('simhash') else None
                await conn.execute('''
                    INSERT INTO pages (url, domain, title, content_hash, simhash, content_type, relevance_score)
                    VALUES ($1, $2, $3, $4, $5, $6, $7)
                    ON CONFLICT (url) DO UPDATE SET
                        title = EXCLUDED.title,
                        content_hash = EXCLUDED.content_hash,
                        simhash = EXCLUDED.simhash,
                        relevance_score = EXCLUDED.relevance_score,
                        crawled_at = CURRENT_TIMESTAMP
                ''', data['url'], data['domain'], data['title'], data['content_hash'],
                    simhash_str, data['content_type'], data['relevance'])
            except Exception as e:
                print(f"[!] DB é”™è¯¯ (page): {e}")
    
    async def _save_site(self, domain: str):
        """ä¿å­˜ç«™ç‚¹åˆ°æ•°æ®åº“"""
        # åªä¿å­˜ .onion åŸŸå
        if not domain.endswith('.onion'):
            return
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            try:
                await conn.execute('''
                    INSERT INTO onion_sites (domain) VALUES ($1)
                    ON CONFLICT (domain) DO NOTHING
                ''', domain)
            except Exception as e:
                print(f"[!] DB é”™è¯¯ (site): {e}")
    
    async def _save_document(self, url: str, content_type: str):
        """ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“"""
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            try:
                await conn.execute('''
                    INSERT INTO documents (url, content_type) VALUES ($1, $2)
                    ON CONFLICT (url) DO NOTHING
                ''', url, content_type)
            except Exception as e:
                print(f"[!] DB é”™è¯¯ (doc): {e}")
    
    def _extract_title(self, html: str) -> str:
        try:
            soup = BeautifulSoup(html, 'html.parser')
            if soup.title and soup.title.string:
                return soup.title.string.strip()[:100]
        except:
            pass
        return "Untitled"
    
    async def sniff_page(self, url: str, semaphore: asyncio.Semaphore) -> set:
        """å¼‚æ­¥å—…æ¢å•ä¸ªé¡µé¢"""
        async with semaphore:
            # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦å·²è®¿é—®
            if url in self.visited_urls:
                return set()
            
            # æ£€æŸ¥åŸŸåæ˜¯å¦è¢«æ‹‰é»‘
            domain = urlparse(url).netloc
            if domain in self.blacklisted_domains:
                return set()
            
            # è·³è¿‡æ¸…ç½‘é•œåƒç«™
            if self._is_clearnet_mirror(domain):
                return set()
            
            new_links = set()
            
            try:
                # 1. HEAD è¯·æ±‚æ¢æµ‹ç±»å‹
                head = await self.engine.head(url, timeout=30)
                content_type = head['headers'].get('Content-Type', '')
                
                # 2. æ–‡æ¡£èµ„æº
                if any(ext in content_type.lower() for ext in ['pdf', 'zip', 'rar', 'octet-stream']):
                    print(f"[!] å‘ç°èµ„æº: {content_type[:20]} @ {url[:50]}")
                    await self._save_document(url, content_type)
                    return new_links
                
                # 3. HTML é¡µé¢
                if 'text/html' in content_type or not content_type:
                    status, ct, body = await self.engine.fetch(url, timeout=60)
                    html = body.decode('utf-8', errors='ignore')
                    
                    # MD5 å»é‡
                    content_hash = hashlib.md5(html.encode()).hexdigest()
                    if content_hash in self.visited_hashes:
                        return new_links
                    
                    # SimHash ç›¸ä¼¼åº¦å»é‡
                    simhash_val = self.simhash.compute(html)
                    if self._is_content_similar(simhash_val):
                        return new_links
                    
                    title = self._extract_title(html)
                    relevance = self._calculate_relevance(html, title)
                    domain = urlparse(url).netloc
                    
                    # ä¿å­˜
                    await self._save_page({
                        'url': url, 'domain': domain, 'title': title,
                        'content_hash': content_hash, 'simhash': simhash_val,
                        'content_type': content_type, 'relevance': relevance
                    })
                    
                    # ä¿å­˜ç«™ç‚¹
                    await self._save_site(domain)
                    
                    # æ›´æ–°å†…å­˜çŠ¶æ€
                    async with self._lock:
                        self.visited_urls.add(url)
                        self.visited_hashes.add(content_hash)
                        self.simhashes.append(simhash_val)
                    
                    marker = "â˜…" if relevance > 0.5 else "+"
                    print(f"[{marker}] {title[:35]}... (r:{relevance:.1f}) @ {url[:45]}")
                    
                    # æå–é“¾æ¥
                    new_links = self.extract_onion_links(html, url)
                    for link in new_links:
                        d = urlparse(link).netloc
                        if d:
                            await self._save_site(d)
                    
                    # æˆåŠŸäº†ï¼Œé‡ç½®è¯¥åŸŸåçš„å¤±è´¥è®¡æ•°
                    if domain in self.domain_fail_count:
                        del self.domain_fail_count[domain]
            
            except asyncio.TimeoutError:
                print(f"[T] è¶…æ—¶: {url[:50]}")
                await self._record_domain_fail(domain)
            except Exception as e:
                err_msg = str(e)
                short_msg = err_msg[:40]
                
                # ä»£ç†è¿æ¥å¤±è´¥ä¸è®¡å…¥åŸŸåå¤±è´¥ï¼ˆæ˜¯ç½‘ç»œé—®é¢˜ï¼Œä¸æ˜¯ç«™ç‚¹é—®é¢˜ï¼‰
                if 'connect to proxy' in err_msg.lower() or 'Errno 22' in err_msg:
                    print(f"[âš ] ä»£ç†æ–­å¼€: {short_msg}")
                    # ä¸è®°å½•åŸŸåå¤±è´¥
                elif 'Cannot connect' not in short_msg and 'Connection refused' not in short_msg:
                    print(f"[âœ—] {url[:40]}... - {short_msg}")
                    await self._record_domain_fail(domain)
            
            return new_links - self.visited_urls
    
    async def crawl_async(self, seeds: list, max_pages: int = 100, concurrency: int = 10):
        """å¼‚æ­¥çˆ¬å–ä¸»å¾ªç¯ï¼Œmax_pages=0 è¡¨ç¤ºæ— é™çˆ¬å–"""
        # åˆå§‹åŒ–æ•°æ®åº“
        await self.init_db()
        
        # æ£€æŸ¥è¿æ¥
        if not await self.engine.check_connection():
            print("[!] Tor æœªè¿æ¥")
            return
        
        queue = list(seeds)
        crawled = 0
        semaphore = asyncio.Semaphore(concurrency)
        unlimited = (max_pages == 0)
        
        target_str = "âˆ" if unlimited else str(max_pages)
        print(f"\n[*] å¼‚æ­¥çˆ¬å–å¯åŠ¨ï¼Œç§å­: {len(seeds)}ï¼Œç›®æ ‡: {target_str}ï¼Œå¹¶å‘: {concurrency}\n")
        
        while queue and (unlimited or crawled < max_pages):
            # å–ä¸€æ‰¹ URL
            if unlimited:
                batch_size = min(concurrency * 2, len(queue))
            else:
                batch_size = min(concurrency * 2, max_pages - crawled, len(queue))
            
            batch = []
            while queue and len(batch) < batch_size:
                url = queue.pop(0)
                domain = urlparse(url).netloc
                # è·³è¿‡å·²è®¿é—®å’Œé»‘åå•åŸŸå
                if url not in self.visited_urls and domain not in self.blacklisted_domains:
                    batch.append(url)
            
            if not batch:
                if not queue:
                    # é˜Ÿåˆ—ç©ºäº†ï¼Œå°è¯•ä»æ•°æ®åº“è¡¥å……
                    new_seeds = await self._get_pending_seeds_async(limit=50)
                    if new_seeds:
                        queue.extend(new_seeds)
                        print(f"[*] é˜Ÿåˆ—ç©ºï¼Œä»æ•°æ®åº“è¡¥å…… {len(new_seeds)} ä¸ªç§å­")
                        continue
                    else:
                        break
                continue
            
            # å¹¶å‘æ‰§è¡Œ
            tasks = [self.sniff_page(url, semaphore) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, set):
                    queue.extend(result)
                    crawled += 1
            
            # å®šæœŸæ‰“å°è¿›åº¦
            if crawled % 50 == 0 and crawled > 0:
                print(f"\n[*] è¿›åº¦: å·²çˆ¬å– {crawled} é¡µï¼Œé˜Ÿåˆ—: {len(queue)}\n")
        
        await self.engine.close()
        print(f"\n[âœ“] çˆ¬å–å®Œæˆï¼Œå…± {crawled} é¡µ")
        
        # å…ˆæ‰“å°ç»Ÿè®¡ï¼Œå†å…³é—­è¿æ¥æ± 
        await self._print_stats_async()
        
        if self._pool:
            await self._pool.close()
    
    async def _get_pending_seeds_async(self, limit: int = 50) -> list:
        """ä»æ•°æ®åº“è·å–å¾…çˆ¬å–çš„ç«™ç‚¹ä½œä¸ºç§å­"""
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            rows = await conn.fetch('''
                SELECT DISTINCT 'http://' || s.domain || '/' as url
                FROM onion_sites s
                WHERE NOT EXISTS (
                    SELECT 1 FROM pages p WHERE p.domain = s.domain
                )
                LIMIT $1
            ''', limit)
            return [row['url'] for row in rows]
    
    async def _get_high_relevance_seeds_async(self, min_score: float = 0.5, limit: int = 20) -> list:
        """è·å–é«˜ç›¸å…³åº¦é¡µé¢çš„åŒåŸŸåå…¶ä»–é¡µé¢ä½œä¸ºç§å­"""
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            rows = await conn.fetch('''
                SELECT domain, MAX(relevance_score) as max_score
                FROM pages 
                WHERE relevance_score >= $1 
                GROUP BY domain
                ORDER BY max_score DESC
                LIMIT $2
            ''', min_score, limit)
            return [f"http://{row['domain']}/" for row in rows]
    
    async def _print_stats_async(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            sites = await conn.fetchval("SELECT COUNT(*) FROM onion_sites")
            pages = await conn.fetchval("SELECT COUNT(*) FROM pages")
            docs = await conn.fetchval("SELECT COUNT(*) FROM documents")
            relevant = await conn.fetchval("SELECT COUNT(*) FROM pages WHERE relevance_score > 0.5")
            top_pages = await conn.fetch('''
                SELECT title, url, relevance_score 
                FROM pages 
                ORDER BY relevance_score DESC 
                LIMIT 5
            ''')
        
        print(f"\n{'='*50}")
        print(f"ğŸ“Š çˆ¬å–ç»Ÿè®¡")
        print(f"{'='*50}")
        print(f"å‘ç°ç«™ç‚¹: {sites}")
        print(f"çˆ¬å–é¡µé¢: {pages}")
        print(f"å‘ç°æ–‡æ¡£: {docs}")
        print(f"é«˜ç›¸å…³é¡µ: {relevant}")
        
        if top_pages:
            print(f"\nğŸ”¥ æœ€ç›¸å…³çš„é¡µé¢:")
            for row in top_pages:
                t = row['title'][:35] if row['title'] else "Untitled"
                print(f"  [{row['relevance_score']:.1f}] {t} - {row['url'][:45]}")
    
    def crawl(self, seeds: list, max_pages: int = 100, concurrency: int = 10):
        """åŒæ­¥å…¥å£"""
        asyncio.run(self.crawl_async(seeds, max_pages, concurrency))
    
    def crawl_continue(self, max_pages: int = 0, concurrency: int = 10):
        """ç»§ç»­çˆ¬å–ï¼šä½¿ç”¨å·²å‘ç°çš„ç«™ç‚¹ä½œä¸ºç§å­ï¼Œmax_pages=0 è¡¨ç¤ºæ— é™"""
        asyncio.run(self._crawl_continue_async(max_pages, concurrency))
    
    async def _crawl_continue_async(self, max_pages: int = 0, concurrency: int = 10):
        """ç»§ç»­çˆ¬å–çš„å¼‚æ­¥å®ç°"""
        await self.init_db()
        
        # ä¼˜å…ˆçˆ¬é«˜ç›¸å…³åº¦ç«™ç‚¹
        seeds = await self._get_high_relevance_seeds_async(min_score=0.5, limit=50)
        # è¡¥å……æœªçˆ¬å–çš„ç«™ç‚¹ï¼ˆå–æ›´å¤šï¼‰
        pending = await self._get_pending_seeds_async(limit=500)
        seeds.extend([s for s in pending if s not in seeds])
        
        if not seeds:
            print("[!] æ²¡æœ‰å¾…çˆ¬å–çš„ç«™ç‚¹ï¼Œè¯·å…ˆè¿è¡Œåˆå§‹çˆ¬å–")
            return
        
        print(f"[*] ä»æ•°æ®åº“åŠ è½½ {len(seeds)} ä¸ªç§å­ç«™ç‚¹")
        await self.crawl_async(seeds, max_pages, concurrency)


if __name__ == "__main__":
    import sys
    
    sniffer = OnionSniffer()
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == '--continue':
        # ç»§ç»­çˆ¬å–æ¨¡å¼ï¼šä½¿ç”¨å·²å‘ç°çš„ç«™ç‚¹ï¼Œé»˜è®¤æ— é™çˆ¬å–
        max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 0
        sniffer.crawl_continue(max_pages=max_pages, concurrency=10)
    else:
        # åˆå§‹çˆ¬å–æ¨¡å¼
        seeds = [
            "http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/index.php/Main_Page",
            "http://juhanurmihxlp77nkq76byazcldy2hlmovfu2epvl5ankdibsot4csyd.onion/",
            "http://duckduckgogg42xjoc72x3sjasowoarfbgcmvfimaftt6twagswzczad.onion/",
            "http://xmh57jrknzkhv6y3ls3ubitzfqnkrwxhopf5aygthi7d6rplyvk3noyd.onion/",
            "http://p53lf57qovyuvwsc6xnrppyply3vtqm7l6pcobkmyqsiofyeznfu5uqd.onion/",
        ]
        # é»˜è®¤æ— é™çˆ¬å–
        max_pages = int(sys.argv[1]) if len(sys.argv) > 1 else 0
        sniffer.crawl(seeds, max_pages=max_pages, concurrency=10)
