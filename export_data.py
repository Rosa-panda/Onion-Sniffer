#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¯¼å‡ºå·¥å…· - å¯¼å‡ºçˆ¬å–çš„æ•°æ®ä¾›åˆ†äº«

ç”¨æ³•ï¼š
    python export_data.py [æ ¼å¼] [è¾“å‡ºæ–‡ä»¶]

æ ¼å¼ï¼š
    csv  - CSV æ ¼å¼ï¼ˆé»˜è®¤ï¼‰
    json - JSON æ ¼å¼
    sql  - SQL INSERT è¯­å¥
    md   - Markdown æ ¼å¼ï¼ˆGitHub å¯ç‚¹å‡»é“¾æ¥ï¼‰

ç¤ºä¾‹ï¼š
    python export_data.py csv onion_data.csv
    python export_data.py json onion_data.json
    python export_data.py md SITES.md
"""
import sys
import json
import csv
import asyncio
import asyncpg
from datetime import datetime

# ä»é…ç½®æ–‡ä»¶åŠ è½½
try:
    from config import PG_CONFIG
except ImportError:
    PG_CONFIG = {
        'host': 'localhost',
        'port': 5432,
        'database': 'onion_data',
        'user': 'postgres',
        'password': 'your_password_here'
    }
    print("[!] è­¦å‘Š: æœªæ‰¾åˆ° config.pyï¼Œè¯·å¤åˆ¶ config.example.py å¹¶é…ç½®")


async def export_csv(output_file: str):
    """å¯¼å‡ºä¸º CSV æ ¼å¼"""
    conn = await asyncpg.connect(**PG_CONFIG)
    
    # å¯¼å‡ºç«™ç‚¹
    sites = await conn.fetch("SELECT domain, first_seen, last_seen, status FROM onion_sites ORDER BY first_seen")
    with open(f"{output_file}_sites.csv", 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['domain', 'first_seen', 'last_seen', 'status'])
        for row in sites:
            writer.writerow([row['domain'], row['first_seen'], row['last_seen'], row['status']])
    print(f"[âœ“] å¯¼å‡º {len(sites)} ä¸ªç«™ç‚¹åˆ° {output_file}_sites.csv")
    
    # å¯¼å‡ºé¡µé¢
    pages = await conn.fetch("""
        SELECT url, domain, title, relevance_score, crawled_at 
        FROM pages ORDER BY relevance_score DESC
    """)
    with open(f"{output_file}_pages.csv", 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['url', 'domain', 'title', 'relevance_score', 'crawled_at'])
        for row in pages:
            writer.writerow([row['url'], row['domain'], row['title'], row['relevance_score'], row['crawled_at']])
    print(f"[âœ“] å¯¼å‡º {len(pages)} ä¸ªé¡µé¢åˆ° {output_file}_pages.csv")
    
    # å¯¼å‡ºæ–‡æ¡£
    docs = await conn.fetch("SELECT url, content_type, downloaded_at FROM documents")
    with open(f"{output_file}_docs.csv", 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['url', 'content_type', 'downloaded_at'])
        for row in docs:
            writer.writerow([row['url'], row['content_type'], row['downloaded_at']])
    print(f"[âœ“] å¯¼å‡º {len(docs)} ä¸ªæ–‡æ¡£åˆ° {output_file}_docs.csv")
    
    await conn.close()


async def export_json(output_file: str):
    """å¯¼å‡ºä¸º JSON æ ¼å¼"""
    conn = await asyncpg.connect(**PG_CONFIG)
    
    sites = await conn.fetch("SELECT domain, first_seen, status FROM onion_sites")
    pages = await conn.fetch("SELECT url, domain, title, relevance_score FROM pages")
    docs = await conn.fetch("SELECT url, content_type FROM documents")
    
    data = {
        'exported_at': datetime.now().isoformat(),
        'stats': {
            'sites': len(sites),
            'pages': len(pages),
            'documents': len(docs)
        },
        'sites': [dict(row) for row in sites],
        'pages': [dict(row) for row in pages],
        'documents': [dict(row) for row in docs]
    }
    
    # å¤„ç† datetime åºåˆ—åŒ–
    def json_serial(obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        raise TypeError(f"Type {type(obj)} not serializable")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=json_serial)
    
    print(f"[âœ“] å¯¼å‡ºåˆ° {output_file}")
    print(f"    ç«™ç‚¹: {len(sites)}")
    print(f"    é¡µé¢: {len(pages)}")
    print(f"    æ–‡æ¡£: {len(docs)}")
    
    await conn.close()


async def export_markdown(output_file: str):
    """å¯¼å‡ºä¸º Markdown æ ¼å¼ï¼ˆå¯åœ¨ GitHub ç›´æ¥ç‚¹å‡»ï¼‰"""
    conn = await asyncpg.connect(**PG_CONFIG)
    
    # æŒ‰åŸŸååˆ†ç»„ï¼Œå–æ¯ä¸ªåŸŸåæœ€é«˜ç›¸å…³åº¦çš„é¡µé¢
    pages = await conn.fetch("""
        SELECT DISTINCT ON (domain) 
            domain, title, relevance_score, url
        FROM pages 
        WHERE title IS NOT NULL AND title != 'Untitled'
        ORDER BY domain, relevance_score DESC
    """)
    
    # ç»Ÿè®¡
    total_sites = await conn.fetchval("SELECT COUNT(DISTINCT domain) FROM pages")
    total_pages = await conn.fetchval("SELECT COUNT(*) FROM pages")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# ğŸ§… Onion Sites Collection\n\n")
        f.write(f"å—…æ¢å™¨å‘ç°çš„ .onion ç«™ç‚¹åˆ—è¡¨ã€‚\n\n")
        f.write(f"- ç‹¬ç«‹ç«™ç‚¹: **{total_sites}**\n")
        f.write(f"- æ€»é¡µé¢æ•°: **{total_pages}**\n")
        f.write(f"- å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
        
        f.write("## âš ï¸ å…è´£å£°æ˜\n\n")
        f.write("è¿™äº›é“¾æ¥ä»…ä¾›å®‰å…¨ç ”ç©¶ä½¿ç”¨ã€‚å¤§éƒ¨åˆ† Tor éšè—æœåŠ¡æ˜¯è¯ˆéª—ã€å¸‚åœºæˆ–åƒåœ¾å†…å®¹ã€‚\n")
        f.write("è®¿é—®éœ€è¦ Tor æµè§ˆå™¨ã€‚\n\n")
        
        f.write("## ç«™ç‚¹åˆ—è¡¨\n\n")
        f.write("| æ ‡é¢˜ | åŸŸå | ç›¸å…³åº¦ |\n")
        f.write("|------|------|--------|\n")
        
        for row in pages:
            # æ¸…ç†æ ‡é¢˜ï¼šç§»é™¤æ¢è¡Œã€è½¬ä¹‰ç®¡é“ç¬¦ã€æˆªæ–­
            title = (row['title'] or 'Untitled')
            title = title.replace('\n', ' ').replace('\r', ' ')
            title = title.replace('|', '-')  # ç›´æ¥æ›¿æ¢ä¸ºæ¨ªæ ï¼Œé¿å…è½¬ä¹‰é—®é¢˜
            title = title.replace('[', '(').replace(']', ')')  # é¿å…ç ´åé“¾æ¥è¯­æ³•
            title = ' '.join(title.split())[:60]  # åˆå¹¶å¤šä½™ç©ºæ ¼å¹¶æˆªæ–­
            
            domain = row['domain'][:50]
            score = row['relevance_score']
            # ç”Ÿæˆå¯ç‚¹å‡»çš„ onion é“¾æ¥
            link = f"http://{row['domain']}/"
            f.write(f"| {title} | [{domain}]({link}) | {score:.1f} |\n")
    
    print(f"[âœ“] å¯¼å‡º {len(pages)} ä¸ªç«™ç‚¹åˆ° {output_file}")
    await conn.close()


async def export_sql(output_file: str):
    """å¯¼å‡ºä¸º SQL INSERT è¯­å¥"""
    conn = await asyncpg.connect(**PG_CONFIG)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("-- Onion Sniffer æ•°æ®å¯¼å‡º\n")
        f.write(f"-- å¯¼å‡ºæ—¶é—´: {datetime.now().isoformat()}\n\n")
        
        # ç«™ç‚¹
        sites = await conn.fetch("SELECT domain, status FROM onion_sites")
        f.write("-- ç«™ç‚¹æ•°æ®\n")
        for row in sites:
            domain = row['domain'].replace("'", "''")
            f.write(f"INSERT INTO onion_sites (domain, status) VALUES ('{domain}', '{row['status']}') ON CONFLICT DO NOTHING;\n")
        
        # é¡µé¢
        pages = await conn.fetch("SELECT url, domain, title, relevance_score FROM pages")
        f.write("\n-- é¡µé¢æ•°æ®\n")
        for row in pages:
            url = row['url'].replace("'", "''")
            domain = row['domain'].replace("'", "''")
            title = (row['title'] or '').replace("'", "''")
            f.write(f"INSERT INTO pages (url, domain, title, relevance_score) VALUES ('{url}', '{domain}', '{title}', {row['relevance_score']}) ON CONFLICT DO NOTHING;\n")
    
    print(f"[âœ“] å¯¼å‡º {len(sites)} ç«™ç‚¹ + {len(pages)} é¡µé¢åˆ° {output_file}")
    
    await conn.close()


async def print_stats():
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    conn = await asyncpg.connect(**PG_CONFIG)
    
    sites = await conn.fetchval("SELECT COUNT(*) FROM onion_sites")
    pages = await conn.fetchval("SELECT COUNT(*) FROM pages")
    docs = await conn.fetchval("SELECT COUNT(*) FROM documents")
    relevant = await conn.fetchval("SELECT COUNT(*) FROM pages WHERE relevance_score > 0.5")
    
    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡")
    print(f"{'='*40}")
    print(f"ç«™ç‚¹æ€»æ•°: {sites}")
    print(f"é¡µé¢æ€»æ•°: {pages}")
    print(f"æ–‡æ¡£èµ„æº: {docs}")
    print(f"é«˜ç›¸å…³é¡µ: {relevant}")
    
    # æŒ‰åŸŸåç»Ÿè®¡
    top_domains = await conn.fetch("""
        SELECT domain, COUNT(*) as cnt 
        FROM pages GROUP BY domain 
        ORDER BY cnt DESC LIMIT 10
    """)
    print(f"\nğŸ” é¡µé¢æœ€å¤šçš„ç«™ç‚¹:")
    for row in top_domains:
        print(f"  {row['cnt']:4d} - {row['domain'][:50]}")
    
    await conn.close()


async def main():
    if len(sys.argv) < 2:
        await print_stats()
        print(__doc__)
        return
    
    fmt = sys.argv[1].lower()
    output = sys.argv[2] if len(sys.argv) > 2 else f"onion_export_{datetime.now().strftime('%Y%m%d')}"
    
    if fmt == 'csv':
        await export_csv(output)
    elif fmt == 'json':
        if not output.endswith('.json'):
            output += '.json'
        await export_json(output)
    elif fmt == 'sql':
        if not output.endswith('.sql'):
            output += '.sql'
        await export_sql(output)
    elif fmt in ('md', 'markdown'):
        if not output.endswith('.md'):
            output += '.md'
        await export_markdown(output)
    elif fmt == 'stats':
        await print_stats()
    else:
        print(f"[!] æœªçŸ¥æ ¼å¼: {fmt}")
        print("æ”¯æŒ: csv, json, sql, md, stats")


if __name__ == "__main__":
    asyncio.run(main())
